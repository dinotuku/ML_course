{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, x_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of 's' label in y is 34.27%\n",
      "The percentage of 'b' label in y is 65.73%\n"
     ]
    }
   ],
   "source": [
    "# check the percentage of two labels\n",
    "print(\"The percentage of 's' label in y is {:.2f}%\".format(100 * y[y == 1].size / y.size))\n",
    "print(\"The percentage of 'b' label in y is {:.2f}%\".format(100 * y[y == -1].size / y.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into training and validation\n",
    "y_train, y_val, x_train, x_val = train_val_split(y, x, 0.2, seed=1)\n",
    "\n",
    "# normalize data using metrics of training data (except PRI_jet_num (22th column) since it is a discrete value)\n",
    "nor_indices = [idx for idx in range(x_train.shape[1]) if idx != 22]\n",
    "nor_x_train = x_train.copy()\n",
    "nor_x_val = x_val.copy()\n",
    "nor_x_test = x_test.copy()\n",
    "\n",
    "x_train_mean = x_train[:, nor_indices].mean(axis=0)\n",
    "x_train_std = x_train[:, nor_indices].std(axis=0)\n",
    "\n",
    "nor_x_train[:, nor_indices] = (nor_x_train[:, nor_indices] - x_train_mean) / x_train_std\n",
    "nor_x_val[:, nor_indices] = (nor_x_val[:, nor_indices] - x_train_mean) / x_train_std\n",
    "nor_x_test[:, nor_indices] = (nor_x_test[:, nor_indices] - x_train_mean) / x_train_std\n",
    "\n",
    "# add all ones column to features for bias term\n",
    "nor_x_train = np.c_[np.ones((nor_x_train.shape[0], 1)), nor_x_train]\n",
    "nor_x_val = np.c_[np.ones((nor_x_val.shape[0], 1)), nor_x_val]\n",
    "nor_x_test = np.c_[np.ones((nor_x_test.shape[0], 1)), nor_x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression using gradient descent\n",
      "Gamma: 0.175\n",
      "Training Loss: 64032330817782258699348455100848597920567209034474802636499185850267362538230598828223933528095207305235637763000162582528.0000 - Training Accuracy: 0.3736\n",
      "Validation Loss: 84521039643137521684413594552378521643441140801189601383227715639707775688040518795695187604554802351077332321329154621440.0000 - Validation Accuracy: 0.3731\n",
      "Gamma: 0.15\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7446\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7441\n",
      "Gamma: 0.1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7446\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7440\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "max_iters = 1000\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "gammas = [0.175, 0.15, 0.1]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Linear regression using gradient descent')\n",
    "for gamma in gammas:\n",
    "    # train model, get weights and loss\n",
    "    weights, train_loss = least_squares_GD(y_train, nor_x_train, initial_w, max_iters, gamma)\n",
    "    val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Gamma:', gamma)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma: 0.15\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_gamma = gammas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best gamma:', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'lr_gd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression using stochastic gradient descent\n",
      "Gamma: 0.01\n",
      "Training Loss: 0.0718 - Training Accuracy: 0.6967\n",
      "Validation Loss: 0.3922 - Validation Accuracy: 0.6987\n",
      "Gamma: 0.005\n",
      "Training Loss: 1.4715 - Training Accuracy: 0.7132\n",
      "Validation Loss: 0.3632 - Validation Accuracy: 0.7144\n",
      "Gamma: 0.001\n",
      "Training Loss: 0.0663 - Training Accuracy: 0.7166\n",
      "Validation Loss: 0.3659 - Validation Accuracy: 0.7171\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "max_iters = 1000\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "gammas = [0.01, 0.005, 0.001]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Linear regression using stochastic gradient descent')\n",
    "for gamma in gammas:\n",
    "    # train model, get weights and loss\n",
    "    weights, train_loss = least_squares_SGD(y_train, nor_x_train, initial_w, max_iters, gamma)\n",
    "    val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Gamma:', gamma)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma: 0.005\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_gamma = gammas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best gamma:', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'lr_sgd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least squares regression using normal equations\n",
      "Training Loss: 0.3395 - Training Accuracy: 0.7452\n",
      "Validation Loss: 0.3394 - Validation Accuracy: 0.7445\n"
     ]
    }
   ],
   "source": [
    "# train model, get weights and loss\n",
    "weights, train_loss = least_squares(y_train, nor_x_train)\n",
    "val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "# make prediction\n",
    "y_train_pred = predict_labels(weights, nor_x_train)\n",
    "y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "# compute accuracy\n",
    "train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "\n",
    "print('Least squares regression using normal equations')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'ls.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression using normal equations\n",
      "Lambda: 0.001\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7445\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7436\n",
      "Lambda: 0.0005\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7447\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7438\n",
      "Lambda: 0.0001\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7447\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7439\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "lambdas = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Ridge regression using normal equations')\n",
    "for lambda_ in lambdas:\n",
    "    # train model, get weights and loss\n",
    "    weights, train_loss = ridge_regression(y_train, nor_x_train, lambda_)\n",
    "    val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Lambda:', lambda_)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_lambda_ = lambdas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best lambda:', best_lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'rr.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change y label from (1, -1) to (1, 0)\n",
    "lg_y_train = np.where(y_train == -1, 0, y_train)\n",
    "lg_y_val = np.where(y_val == -1, 0, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using gradient descent\n",
      "Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7507\n",
      "Validation Loss: 0.4979 - Validation Accuracy: 0.7495\n",
      "Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7507\n",
      "Validation Loss: 0.4979 - Validation Accuracy: 0.7496\n",
      "Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7507\n",
      "Validation Loss: 0.4980 - Validation Accuracy: 0.7497\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "max_iters = 1000\n",
    "gammas = [1, 0.75, 0.5]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Logistic regression using gradient descent')\n",
    "for gamma in gammas:\n",
    "    # train model, get weights and loss\n",
    "    weights, loss = logistic_regression(lg_y_train, nor_x_train, initial_w, max_iters, gamma)\n",
    "    val_loss = compute_lg_loss(lg_y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_lg_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_lg_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(lg_y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(lg_y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Gamma:', gamma)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma: 1\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_gamma = gammas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best gamma:', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_lg_labels(weights, nor_x_test)\n",
    "y_test_pred[y_test_pred == 0] = -1\n",
    "OUTPUT_PATH = 'lg_gd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using gradient descent\n",
      "Lambda: 0.01 - Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7310\n",
      "Validation Loss: 0.5432 - Validation Accuracy: 0.7299\n",
      "Lambda: 0.01 - Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7467\n",
      "Validation Loss: 0.5056 - Validation Accuracy: 0.7463\n",
      "Lambda: 0.01 - Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7467\n",
      "Validation Loss: 0.5056 - Validation Accuracy: 0.7464\n",
      "Lambda: 0.005 - Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7431\n",
      "Validation Loss: 0.5195 - Validation Accuracy: 0.7411\n",
      "Lambda: 0.005 - Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7486\n",
      "Validation Loss: 0.5014 - Validation Accuracy: 0.7470\n",
      "Lambda: 0.005 - Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7486\n",
      "Validation Loss: 0.5014 - Validation Accuracy: 0.7471\n",
      "Lambda: 0.001 - Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7503\n",
      "Validation Loss: 0.4982 - Validation Accuracy: 0.7489\n",
      "Lambda: 0.001 - Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7502\n",
      "Validation Loss: 0.4982 - Validation Accuracy: 0.7489\n",
      "Lambda: 0.001 - Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7502\n",
      "Validation Loss: 0.4983 - Validation Accuracy: 0.7489\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "lambdas = [0.01, 0.005, 0.001]\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "max_iters = 1000\n",
    "gammas = [1, 0.75, 0.5]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Logistic regression using gradient descent')\n",
    "for lambda_ in lambdas:\n",
    "    for gamma in gammas:\n",
    "        # train model, get weights and loss\n",
    "        weights, loss = reg_logistic_regression(lg_y_train, nor_x_train, lambda_, initial_w, max_iters, gamma)\n",
    "        val_loss = compute_lg_loss(lg_y_val, nor_x_val, weights)\n",
    "\n",
    "        # make prediction\n",
    "        y_train_pred = predict_lg_labels(weights, nor_x_train)\n",
    "        y_val_pred = predict_lg_labels(weights, nor_x_val)\n",
    "\n",
    "        # compute accuracy\n",
    "        train_acc = compute_accuracy(lg_y_train, y_train_pred)\n",
    "        val_acc = compute_accuracy(lg_y_val, y_val_pred)\n",
    "        \n",
    "        # store weights and validation loss\n",
    "        weights_history.append(weights)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        print(\"Lambda: {} - Gamma: {}\".format(lambda_, gamma))\n",
    "        print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "        print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.001 - Best gamma: 1\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_lambda_ = lambdas[int(best_idx / len(lambdas))]\n",
    "best_gamma = gammas[best_idx % len(gammas)]\n",
    "weights = weights_history[best_idx]\n",
    "print(\"Best lambda: {} - Best gamma: {}\".format(best_lambda_, best_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_lg_labels(weights, nor_x_test)\n",
    "y_test_pred[y_test_pred == 0] = -1\n",
    "OUTPUT_PATH = 'lg_sgd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 30)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_tra_accs = [0.7446, 0.7132, 0.7452, 0.7447, 0.7507, 0.7503]\n",
    "best_val_accs = [0.7441, 0.7144, 0.7445, 0.7439, 0.7497, 0.7489]\n",
    "best_test_accs = [0.744, 0.713, 0.745, 0.745, 0.751, 0.751]\n",
    "x = np.arange(len(best_tra_accs))\n",
    "plt.bar(x, best_test_accs, width=0.7, color=sns.color_palette('deep'))\n",
    "for i, v in enumerate(best_test_accs):\n",
    "    plt.text(x[i] - 0.28, v + 0.002, str(v))\n",
    "plt.xticks(x, ('LR_SD', 'LR_SGD', 'LS', 'RR', 'LG', 'RLG'))\n",
    "plt.ylim(bottom=0.7, top=0.76)\n",
    "plt.xlabel('ML Methods')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change y label from (1, -1) to (1, 0)\n",
    "nn_x_train = nor_x_train[:, 1:]\n",
    "nn_x_val = nor_x_val[:, 1:]\n",
    "nn_x_test = nor_x_test[:, 1:]\n",
    "nn_y_train = np.where(y_train == -1, 0, y_train)\n",
    "nn_y_val = np.where(y_val == -1, 0, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 - 1.0s - loss: 0.4240 - acc: 0.8028 - val_loss: 0.3910 - val_acc: 0.8220\n",
      "Saving best model (epoch 3, val_loss: 0.3745)\n",
      "Saving best model (epoch 4, val_loss: 0.3713)\n",
      "Saving best model (epoch 5, val_loss: 0.3705)\n",
      "Saving best model (epoch 7, val_loss: 0.3694)\n",
      "Saving best model (epoch 8, val_acc: 0.8339)\n",
      "Saving best model (epoch 9, val_acc: 0.8349)\n",
      "Saving best model (epoch 9, val_loss: 0.3661)\n",
      "Saving best model (epoch 11, val_loss: 0.3649)\n",
      "Saving best model (epoch 13, val_acc: 0.8356)\n",
      "Saving best model (epoch 15, val_acc: 0.8362)\n",
      "Saving best model (epoch 15, val_loss: 0.3638)\n",
      "Saving best model (epoch 17, val_acc: 0.8371)\n",
      "Saving best model (epoch 18, val_loss: 0.3634)\n",
      "Saving best model (epoch 26, val_acc: 0.8375)\n",
      "Saving best model (epoch 26, val_loss: 0.3632)\n",
      "Saving best model (epoch 30, val_loss: 0.3630)\n",
      "Saving best model (epoch 33, val_acc: 0.8375)\n",
      "Epoch: 00050 - 50.9s - loss: 0.3535 - acc: 0.8414 - val_loss: 0.3641 - val_acc: 0.8361\n",
      "Epoch: 00100 - 44.1s - loss: 0.3491 - acc: 0.8439 - val_loss: 0.3671 - val_acc: 0.8338\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "batch_size = 250\n",
    "\n",
    "# train model, get model parameters\n",
    "nn_params = train(nn_x_train.T, nn_y_train[np.newaxis,:], nn_x_val.T, nn_y_val[np.newaxis,:],\n",
    "                  NN_ARCHITECTURE, epochs, lr, batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = np.load('best_acc.npy', allow_pickle=True).item()\n",
    "# nn_params = np.load('best_loss.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep neural network\n",
      "Training Loss: 0.3528 - Training Accuracy: 0.8419\n",
      "Validation Loss: 0.3632 - Validation Accuracy: 0.8375\n"
     ]
    }
   ],
   "source": [
    "# get final output of the model\n",
    "y_train_hat, _ = full_forward_propagation(nn_x_train.T, nn_params, NN_ARCHITECTURE)\n",
    "y_val_hat, _ = full_forward_propagation(nn_x_val.T, nn_params, NN_ARCHITECTURE)\n",
    "\n",
    "# compute loss and accuracy\n",
    "train_loss = compute_nn_loss(y_train_hat, nn_y_train.reshape(nn_y_train.shape[0], 1).T)\n",
    "val_loss = compute_nn_loss(y_val_hat, nn_y_val.reshape(nn_y_val.shape[0], 1).T)\n",
    "\n",
    "train_acc = compute_nn_accuracy(y_train_hat, nn_y_train.reshape(nn_y_train.shape[0], 1).T)\n",
    "val_acc = compute_nn_accuracy(y_val_hat, nn_y_val.reshape(nn_y_val.shape[0], 1).T)\n",
    "\n",
    "print('Deep neural network')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_hat, _ = full_forward_propagation(nn_x_test.T, nn_params, NN_ARCHITECTURE)\n",
    "y_test_pred = np.where(np.squeeze(y_test_hat.T) > 0.5, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'nn.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
